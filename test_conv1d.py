import torch
import torch.nn.functional as F
from typing import Optional, Tuple
import logging

# Log settings
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("TestScript")
torch.manual_seed(42)

# ==========================================
# 1. SGLang åŸå§‹å®ç°
# ==========================================
class SGLangImpl:
    def torch_causal_conv1d_update_npu(
        self,
        hidden_state: torch.Tensor,
        conv_state: torch.Tensor,
        weight: torch.Tensor,
        conv_state_update: Optional[torch.Tensor] = None,
        bias: Optional[torch.Tensor] = None,
    ):
        bsz, hidden_size, seq_len = hidden_state.shape
        state_len = conv_state.shape[-1]

        hidden_states_new = torch.cat([conv_state, hidden_state], dim=-1).to(
            weight.dtype
        )

        if conv_state_update is not None:
            for i in range(seq_len):
                end = i - seq_len + 1
                start = end - state_len
                slice_range = slice(start, end if end != 0 else None)
                conv_state_update[:, i] = hidden_states_new[:, :, slice_range]
        else:
            conv_state_update = hidden_states_new[:, :, -state_len:]

        kernel_size = weight.shape[-1]
        windows = hidden_states_new.unfold(-1, kernel_size, 1)

        # [æ³¨æ„] è¿™é‡Œå‡è®¾ weight æ˜¯ 2D [H, K]
        out = (windows * weight[None, :, None, :]).sum(dim=-1)

        if bias is not None:
            out = out + bias[None, :, None]

        out = F.silu(out[:, :, -seq_len:])
        out = out.to(hidden_state.dtype)
        conv_state = conv_state.transpose(1, 2)
        return out, conv_state_update

# ==========================================
# 2. vLLM é£æ ¼ä¿®æ­£ç‰ˆ (V3)
# ==========================================
def vllm_causal_conv1d_update_v3(
    hidden_state: torch.Tensor,
    conv_state: torch.Tensor,
    weight: torch.Tensor,
    conv_state_indices: torch.Tensor,
    bias: Optional[torch.Tensor] = None,
    activation: bool = True,
) -> Tuple[torch.Tensor, torch.Tensor]:
    hidden_state = hidden_state.transpose(1, 2)
    weight = weight.transpose(0, 1)
    conv_state = conv_state.transpose(1, 2)
    bsz, hidden_size, seq_len = hidden_state.shape
    kernel_size = weight.shape[-1]

    # é€»è¾‘: (K-1) + (L-1). ä¸¢å¼ƒæœ€è€çš„å†å² (H1), ä¿ç•™æœ€æ–°çš„è¾“å…¥ (C)
    target_state_len = (kernel_size - 1) + (seq_len - 1)

    full_context = torch.cat([conv_state[conv_state_indices], hidden_state], dim=-1).to(weight.dtype)

    # è®¡ç®— output
    computation_input = full_context[:, :, -(kernel_size - 1 + seq_len):]
    windows = computation_input.unfold(-1, kernel_size, 1)

    # åŒæ ·å‡è®¾ weight æ˜¯ 2D [H, K]
    out = (windows * weight[None, :, None, :]).sum(dim=-1)

    if bias is not None:
        out = out + bias[None, :, None]

    if activation:
        out = F.silu(out)

    out = out.to(hidden_state.dtype)

    # æ›´æ–° State: ä¿ç•™æœ€å target_state_len é•¿åº¦
    if target_state_len > 0:
        new_conv_state = full_context[:, :, -target_state_len:]
    else:
        new_conv_state = torch.empty(bsz, hidden_size, 0, device=hidden_state.device, dtype=hidden_state.dtype)
    new_conv_state = new_conv_state.transpose(1, 2)
    return out, new_conv_state

# ==========================================
# 3. ä¿®å¤åçš„æµ‹è¯•å‡½æ•°
# ==========================================
def test_correctness_fixed():
    # --- Config ---
    BSZ = 8
    HIDDEN_SIZE = 4096
    SEQ_LEN = 2
    KERNEL_SIZE = 3
    CACHE_LEN = 65
    DTYPE = torch.bfloat16
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

    print(f"Running Test (Fixed Logic) on {DEVICE}...")

    # [FIXED HERE] Weight shape changed to [H, K] (2D)
    weight = torch.randn(KERNEL_SIZE, HIDDEN_SIZE, device=DEVICE, dtype=DTYPE)
    bias = torch.randn(HIDDEN_SIZE, device=DEVICE, dtype=DTYPE)

    hidden_state = torch.randn(BSZ, SEQ_LEN, HIDDEN_SIZE, device=DEVICE, dtype=DTYPE)
    conv_state_init = torch.randn(CACHE_LEN, KERNEL_SIZE - 1, HIDDEN_SIZE, device=DEVICE, dtype=DTYPE)

    conv_state_indices = torch.arange(BSZ, device=hidden_state.device)

    # --- SGLang Execution ---
    sglang_model = SGLangImpl()
    sglang_cache_buffer = torch.zeros(
        BSZ, SEQ_LEN, HIDDEN_SIZE, KERNEL_SIZE - 1,
        device=DEVICE, dtype=DTYPE
    )

    out_sg, final_buffer_sg = sglang_model.torch_causal_conv1d_update_npu(
        hidden_state=hidden_state.transpose(1, 2),
        conv_state=conv_state_init.transpose(1, 2)[conv_state_indices],
        weight=weight.transpose(0, 1),
        conv_state_update=sglang_cache_buffer,
        bias=bias
    )

    # --- vLLM Execution ---
    out_vl, state_vl = vllm_causal_conv1d_update_v3(
        hidden_state=hidden_state,
        conv_state=conv_state_init,
        weight=weight,
        bias=bias,
        conv_state_indices=conv_state_indices,
        activation="silu"
    )

    # --- Validation ---

    # 1. Output Check
    try:
        torch.testing.assert_close(out_sg, out_vl, rtol=1e-5, atol=1e-5)
        print("âœ… Outputs match perfectly!")
    except AssertionError as e:
        print("âŒ Outputs mismatched!")
        print(e)
        return

    # 2. State Length Check
    state_vl_t = state_vl.transpose(1, 2)
    expected_len = (KERNEL_SIZE - 1) + (SEQ_LEN - 1)
    print(f"State Shapes -> SGLang Buffer: {final_buffer_sg.shape}, vLLM State: {state_vl_t.shape}")
    assert state_vl_t.shape[-1] == expected_len, f"Length mismatch: {state_vl_t.shape[-1]} vs {expected_len}"
    print(f"âœ… vLLM State length is correct: {expected_len}")

    # 3. Cache Content Check
    print("--- Verifying Cache Slices ---")
    match_count = 0

    for i in range(SEQ_LEN):
        # SGLang: History used to predict i+1
        sg_slice = final_buffer_sg[:, i, :, :]

        # vLLM: Reconstruct window from continuous state
        # vLLM state ends with the token (SEQ_LEN - 1).
        # Token i is (SEQ_LEN - 1 - i) steps away from the end.
        end_idx = state_vl_t.shape[-1] - (SEQ_LEN - 1 - i)
        start_idx = end_idx - (KERNEL_SIZE - 1)

        vl_slice = state_vl_t[:, :, start_idx : end_idx]

        try:
            torch.testing.assert_close(sg_slice, vl_slice, rtol=1e-5, atol=1e-5)
            match_count += 1
        except AssertionError:
            print(f"âŒ Mismatch at index {i}")
            break

    if match_count == SEQ_LEN:
        print(f"âœ… Verified {match_count} intermediate states (Full Coverage).")
    else:
        print("âŒ Cache content verification failed.")

# ==========================================
# 4. NPUç®—å­æµ‹è¯•
# ==========================================
def test_npu_causal_conv1d_update():
    """æµ‹è¯• NPU causal_conv1d_update ç®—å­"""
    try:
        import torch_npu
    except ImportError as e:
        print(f"âš ï¸  Skipping NPU test (import failed): {e}")
        return

    # Import sgl_kernel_npu to ensure operator registration
    try:
        import sgl_kernel_npu
    except ImportError as e:
        print(f"âš ï¸  Skipping NPU test (sgl_kernel_npu import failed): {e}")
        return

    # Check NPU availability
    try:
        if not (hasattr(torch_npu, 'npu') and torch.npu.device_count() > 0):
            print("âš ï¸  NPU not available, skipping NPU test")
            return
    except Exception as e:
        print(f"âš ï¸  Failed to check NPU availability: {e}")
        return

    # Verify operator is registered
    if not hasattr(torch.ops.npu, 'causal_conv1d_update'):
        print("âš ï¸  causal_conv1d_update operator not registered!")
        print(f"Available npu ops: {[op for op in dir(torch.ops.npu) if not op.startswith('_')][:10]}")
        return

    # --- Config ---
    BSZ = 4
    HIDDEN_SIZE = 1024  # ä½¿ç”¨è¾ƒå°çš„éšè—å¤§å°ä»¥åŠ å¿«æµ‹è¯•
    SEQ_LEN = 2
    KERNEL_SIZE = 3
    CACHE_LEN = 10
    DTYPE = torch.bfloat16
    DEVICE = "npu"

    print(f"\n{'='*50}")
    print(f"Testing NPU causal_conv1d_update on {DEVICE}")
    print(f"{'='*50}")

    # åˆ›å»ºå‚æ•°
    weight = torch.randn(KERNEL_SIZE, HIDDEN_SIZE, device=DEVICE, dtype=DTYPE)
    bias = torch.randn(HIDDEN_SIZE, device=DEVICE, dtype=DTYPE)
    hidden_state = torch.randn(BSZ, SEQ_LEN, HIDDEN_SIZE, device=DEVICE, dtype=DTYPE)
    conv_state_init = torch.randn(CACHE_LEN, KERNEL_SIZE - 1, HIDDEN_SIZE, device=DEVICE, dtype=DTYPE)
    conv_state_indices = torch.arange(BSZ, device=DEVICE, dtype=torch.int32)

    # ç”¨äºç´¢å¼•çš„å¯é€‰å¼ é‡
    query_start_loc = torch.tensor([0, SEQ_LEN, 2*SEQ_LEN, 3*SEQ_LEN], device=DEVICE, dtype=torch.int32)

    # --- vLLM Execution (CPU/CUDA reference) ---
    out_vl, state_vl = vllm_causal_conv1d_update_v3(
        hidden_state=hidden_state.cpu(),
        conv_state=conv_state_init.cpu(),
        weight=weight.cpu(),
        bias=bias.cpu(),
        conv_state_indices=conv_state_indices.cpu(),
        activation=True
    )

    # --- NPU Execution ---
    print(f"Input shapes: x={hidden_state.shape}, weight={weight.shape}, conv_state={conv_state_init.shape}")
    print(f"Calling torch.ops.npu.causal_conv1d_update...")

    # å…‹éš†conv_stateå› ä¸ºNPUä¼šåŸåœ°ä¿®æ”¹
    conv_state_npu = conv_state_init.clone()

    try:
        out_npu = torch.ops.npu.causal_conv1d_update(
            x=hidden_state,
            weight=weight,
            conv_state=conv_state_npu,
            conv_state_indices=conv_state_indices,
            bias=bias,
            num_accepted_tokens=torch.empty(0, device=DEVICE, dtype=torch.int32),  # å¯é€‰å‚æ•°ï¼Œä¼ ç©º
            query_start_loc=torch.empty(0, device=DEVICE, dtype=torch.int32),  # å¯é€‰å‚æ•°ï¼Œä¼ ç©º
            activation_mode=True,
            pad_slot_id=-1
        )

        print(f"âœ… NPU kernel executed successfully!")
        print(f"Output shape: {out_npu.shape}")

        # --- éªŒè¯ ---
        # å°†NPUç»“æœè½¬å›CPUè¿›è¡Œæ¯”è¾ƒ
        out_npu_cpu = out_npu.cpu()

        # è¾“å‡ºå½¢çŠ¶æ£€æŸ¥ - vLLMè¾“å‡ºæ˜¯ [batch, dim, seq_len]ï¼ŒNPUè¾“å‡ºæ˜¯ [batch, seq_len, dim]
        out_vl_transposed = out_vl.transpose(1, 2)  # è½¬æ¢ä¸º [batch, seq_len, dim] è¿›è¡Œæ¯”è¾ƒ

        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
        assert out_npu_cpu.shape == out_vl_transposed.shape, \
            f"Output shape mismatch: {out_npu_cpu.shape} vs {out_vl_transposed.shape}"
        print(f"âœ… Output shape matched: {out_npu_cpu.shape}")

        # æ³¨æ„ï¼šç”±äºç®—å­å®ç°å·®å¼‚ï¼Œæ•°å€¼ç»“æœå¯èƒ½ä¼šæœ‰å¾®å°å·®å¼‚
        # ä¸»è¦éªŒè¯ç®—å­å¯ä»¥æ­£ç¡®æ‰§è¡Œå¹¶äº§ç”Ÿåˆç†çš„è¾“å‡º
        print(f"NPU output - shape: {out_npu_cpu.shape}, dtype: {out_npu_cpu.dtype}, mean: {out_npu_cpu.mean().item():.6f}")
        print(f"vLLM output (transposed) - shape: {out_vl_transposed.shape}, dtype: {out_vl_transposed.dtype}, mean: {out_vl_transposed.mean().item():.6f}")

        # éªŒè¯è¾“å‡ºä¸æ˜¯å…¨é›¶
        assert not torch.all(out_npu_cpu == 0), "NPU output is all zeros!"
        print(f"âœ… NPU output is not all zeros")

        print(f"\nğŸ‰ NPU causal_conv1d_update test passed!")

    except Exception as e:
        print(f"âŒ NPU test failed with error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    print("="*60)
    print("Running test_correctness_fixed (CPU/CUDA reference)")
    print("="*60)
    test_correctness_fixed()

    print("\n" + "="*60)
    print("Running test_npu_causal_conv1d_update (NPU kernel)")
    print("="*60)
    test_npu_causal_conv1d_update()
